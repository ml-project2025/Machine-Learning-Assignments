# train_emnist.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, ConcatDataset
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import json
from datetime import datetime
import argparse
import numpy as np

# ØªØ¹Ø·ÙŠÙ„ GPU Ù„ØªÙØ§Ø¯ÙŠ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªÙˆØ§ÙÙ‚
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''

# -------------------------
# 1ï¸âƒ£ Ù†Ù…ÙˆØ°Ø¬ Improved CNN
# -------------------------
class ImprovedDigitCNN(nn.Module):
    def __init__(self, num_classes=10):  # Ø§ÙØªØ±Ø§Ø¶ÙŠ 10 ÙØ¦Ø§Øª Ù„Ù„Ø£Ø±Ù‚Ø§Ù…
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = nn.functional.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = nn.functional.relu(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

# -------------------------
# 2ï¸âƒ£ DataLoader Ù…Ø¹Ø¯Ù„ Ø¨Ø¯ÙˆÙ† Ø¥Ù†ØªØ±Ù†Øª
# -------------------------
class EMNISTDataLoader:
    def __init__(self, batch_size=64, use_mnist_only=True, use_letters_only=False):
        self.batch_size = batch_size
        self.use_mnist_only = use_mnist_only
        self.use_letters_only = use_letters_only
        self.transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])

    def get_data_loaders(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„Ø¥Ù†ØªØ±Ù†Øª"""
        try:
            if self.use_mnist_only:
                print("ğŸ“¦ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª MNIST ÙÙ‚Ø· (0-9)")
                return self._load_mnist_only()
            elif self.use_letters_only:
                print("ğŸ“¦ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø­Ø±Ù ÙÙ‚Ø· (A-Z)")
                return self._load_letters_only()
            else:
                print("ğŸ“¦ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª EMNIST ÙƒØ§Ù…Ù„Ø©...")
                return self._load_emnist_with_fallback()
                
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            print("ğŸ”„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª MNIST ÙƒØ¨Ø¯ÙŠÙ„")
            return self._load_mnist_only()

    def _load_mnist_only(self):
        """ØªØ­Ù…ÙŠÙ„ MNIST ÙÙ‚Ø· (ÙŠØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ø¥Ù†ØªØ±Ù†Øª)"""
        train_dataset = datasets.MNIST(
            root='./data',
            train=True,
            download=False,
            transform=self.transform
        )
        test_dataset = datasets.MNIST(
            root='./data',
            train=False,
            download=False,
            transform=self.transform
        )

        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ MNIST: {len(train_dataset)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨ØŒ {len(test_dataset)} Ø¹ÙŠÙ†Ø© Ø§Ø®ØªØ¨Ø§Ø±")
        return train_loader, test_loader

    def _load_letters_only(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù ÙÙ‚Ø· Ù…Ù† EMNIST"""
        try:
            train_dataset = datasets.EMNIST(
                root='./data',
                split='letters',
                train=True,
                download=False,
                transform=self.transform
            )
            test_dataset = datasets.EMNIST(
                root='./data',
                split='letters',
                train=False,
                download=False,
                transform=self.transform
            )
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù„ØªØ¨Ø¯Ø£ Ù…Ù† 0
            train_dataset.targets = train_dataset.targets - 1
            test_dataset.targets = test_dataset.targets - 1
            
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù: {len(train_dataset)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨ØŒ {len(test_dataset)} Ø¹ÙŠÙ†Ø© Ø§Ø®ØªØ¨Ø§Ø±")
            
        except (RuntimeError, FileNotFoundError):
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø­Ø±ÙØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… MNIST ÙƒØ¨Ø¯ÙŠÙ„")
            return self._load_mnist_only()

        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        return train_loader, test_loader

    def _load_emnist_with_fallback(self):
        """Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ EMNIST Ù…Ø¹ ÙˆØ¬ÙˆØ¯ Ø¨Ø¯ÙŠÙ„ Ø¥Ø°Ø§ ÙØ´Ù„"""
        try:
            train_dataset = datasets.EMNIST(
                root='./data',
                split='byclass',
                train=True,
                download=False,
                transform=self.transform
            )
            test_dataset = datasets.EMNIST(
                root='./data',
                split='byclass',
                train=False,
                download=False,
                transform=self.transform
            )
            
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ EMNIST: {len(train_dataset)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨ØŒ {len(test_dataset)} Ø¹ÙŠÙ†Ø© Ø§Ø®ØªØ¨Ø§Ø±")
            
        except (RuntimeError, FileNotFoundError):
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª EMNISTØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… MNIST ÙƒØ¨Ø¯ÙŠÙ„")
            return self._load_mnist_only()

        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        return train_loader, test_loader

    def check_data_exists(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ù„ÙŠØ§Ù‹"""
        mnist_path = './data/MNIST'
        emnist_path = './data/EMNIST'
        
        mnist_exists = os.path.exists(mnist_path)
        emnist_exists = os.path.exists(emnist_path)
        
        print(f"ğŸ“ MNIST Ù…ÙˆØ¬ÙˆØ¯: {mnist_exists}")
        print(f"ğŸ“ EMNIST Ù…ÙˆØ¬ÙˆØ¯: {emnist_exists}")
        
        return mnist_exists or emnist_exists

# -------------------------
# 3ï¸âƒ£ Trainer Ù…Ø¹Ø¯Ù„
# -------------------------
class DigitTrainer:
    def __init__(self, learning_rate=0.001, use_mnist_only=True, use_letters_only=False):
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ø¯Ø§Ø¦Ù…Ø§Ù‹
        self.device = torch.device("cpu")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
        if use_mnist_only:
            num_classes = 10  # Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·
        elif use_letters_only:
            num_classes = 26  # Ø£Ø­Ø±Ù ÙÙ‚Ø·
        else:
            num_classes = 62  # Ø§Ù„ÙƒÙ„
        
        self.model = ImprovedDigitCNN(num_classes=num_classes).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)
        self.criterion = nn.CrossEntropyLoss()
        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', patience=3, factor=0.5)
        self.train_losses, self.val_losses = [], []
        self.train_accuracies, self.val_accuracies = [], []
        self.use_mnist_only = use_mnist_only
        self.use_letters_only = use_letters_only

        print(f"ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ù‡Ø§Ø²: {self.device}")
        if self.use_mnist_only:
            print("ğŸ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø· (0-9)")
        elif self.use_letters_only:
            print("ğŸ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù ÙÙ‚Ø· (A-Z)")
        else:
            print("ğŸ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø­Ø±ÙˆÙ (0-9 + A-Z)")

    def train_epoch(self, loader):
        self.model.train()
        running_loss, correct, total = 0.0, 0, 0
        
        progress_bar = tqdm(loader, desc="Ø§Ù„ØªØ¯Ø±ÙŠØ¨", leave=False)
        for data, target in progress_bar:
            data, target = data.to(self.device), target.to(self.device)
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100. * correct / total:.2f}%'
            })
            
        return running_loss / len(loader), 100. * correct / total

    def validate_epoch(self, loader):
        self.model.eval()
        running_loss, correct, total = 0.0, 0, 0
        
        with torch.no_grad():
            for data, target in tqdm(loader, desc="Ø§Ù„ØªØ­Ù‚Ù‚", leave=False):
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                running_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
        return running_loss / len(loader), 100. * correct / total

    def train(self, num_epochs=10, save_path='./saved_models'):
        data_loader = EMNISTDataLoader(
            use_mnist_only=self.use_mnist_only,
            use_letters_only=self.use_letters_only
        )
        
        if not data_loader.check_data_exists():
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨!")
            return
            
        train_loader, val_loader = data_loader.get_data_loaders()
        best_val_acc = 0.0
        os.makedirs(save_path, exist_ok=True)

        print(f"\nğŸ¬ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù…Ø¯Ø© {num_epochs} Ø­Ù‚Ø¨Ø§Øª")
        print("=" * 50)

        for epoch in range(num_epochs):
            print(f"\nğŸ“ Ø§Ù„Ø­Ù‚Ø¨Ø© {epoch+1}/{num_epochs}")
            
            train_loss, train_acc = self.train_epoch(train_loader)
            val_loss, val_acc = self.validate_epoch(val_loader)
            
            self.scheduler.step(val_loss)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            print(f"ğŸ“Š Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {train_loss:.4f}, Ø§Ù„Ø¯Ù‚Ø©: {train_acc:.2f}%")
            print(f"ğŸ“ˆ Ø§Ù„ØªØ­Ù‚Ù‚ - Ø§Ù„Ø®Ø³Ø§Ø±Ø©: {val_loss:.4f}, Ø§Ù„Ø¯Ù‚Ø©: {val_acc:.2f}%")
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self.save_model(save_path)
                print(f"âœ… Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© ØªØ­Ù‚Ù‚: {val_acc:.2f}%")

        print(f"\nğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø£ÙØ¶Ù„ Ø¯Ù‚Ø© ØªØ­Ù‚Ù‚: {best_val_acc:.2f}%")
        self.plot_training_history()
        self.save_training_results()

    def save_model(self, save_path):
        # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
        if self.use_mnist_only:
            num_classes = 10
        elif self.use_letters_only:
            num_classes = 26
        else:
            num_classes = 62
            
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_type': 'ImprovedDigitCNN',
            'num_classes': num_classes,
            'accuracy': self.val_accuracies[-1] if self.val_accuracies else 0
        }, os.path.join(save_path, 'best_digit_model.pth'))

    def plot_training_history(self):
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨', color='blue', linewidth=2)
        plt.plot(self.val_losses, label='Ø®Ø³Ø§Ø±Ø© Ø§Ù„ØªØ­Ù‚Ù‚', color='red', linewidth=2)
        plt.xlabel('Ø§Ù„Ø­Ù‚Ø¨Ø©')
        plt.ylabel('Ø§Ù„Ø®Ø³Ø§Ø±Ø©')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        plt.plot(self.train_accuracies, label='Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨', color='green', linewidth=2)
        plt.plot(self.val_accuracies, label='Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚', color='orange', linewidth=2)
        plt.xlabel('Ø§Ù„Ø­Ù‚Ø¨Ø©')
        plt.ylabel('Ø§Ù„Ø¯Ù‚Ø© (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('./saved_models/training_history.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š ØªÙ… Ø­ÙØ¸ Ù…Ø®Ø·Ø· Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

    def save_training_results(self):
        results = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'final_train_accuracy': self.train_accuracies[-1] if self.train_accuracies else 0,
            'final_val_accuracy': self.val_accuracies[-1] if self.val_accuracies else 0,
            'timestamp': datetime.now().isoformat(),
            'device': str(self.device),
            'use_mnist_only': self.use_mnist_only,
            'use_letters_only': self.use_letters_only
        }
        
        with open('./saved_models/training_results.json', 'w') as f:
            json.dump(results, f, indent=4)
        
        print("ğŸ“ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

# -------------------------
# 4ï¸âƒ£ Main Ù…Ø¹Ø¯Ù„
# -------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø­Ø±ÙˆÙ')
    parser.add_argument("--epochs", type=int, default=10, help="Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 10)")
    parser.add_argument("--lr", type=float, default=0.001, help="Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 0.001)")
    parser.add_argument("--use-mnist-only", action='store_true', help="Ø§Ø³ØªØ®Ø¯Ø§Ù… MNIST ÙÙ‚Ø· (0-9)")
    parser.add_argument("--use-letters-only", action='store_true', help="Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø­Ø±Ù ÙÙ‚Ø· (A-Z)")
    
    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ¤– Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø­Ø±ÙˆÙ")
    print("=" * 60)

    trainer = DigitTrainer(
        learning_rate=args.lr,
        use_mnist_only=args.use_mnist_only,
        use_letters_only=args.use_letters_only
    )
    
    trainer.train(num_epochs=args.epochs)